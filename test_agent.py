from langchain.tools import tool
import google.generativeai as genai
from geopy.geocoders import Nominatim
import time
import pandas as pd
import osmnx as ox
from models.trip_price_class import TripPricePredictor
import heapq
from tools import *
from dotenv import load_dotenv
import os
import pickle
import re
import json

# Load environment variables
load_dotenv()

# Cache the graph to avoid reprocessing each run
CACHE_GRAPH_PATH = "graph_cache.pkl"
if os.path.exists(CACHE_GRAPH_PATH):
    with open(CACHE_GRAPH_PATH, "rb") as f:
        g = pickle.load(f)
else:
    g = ox.graph_from_xml("labeled.osm", bidirectional=True, simplify=True)
    g = attach_trips_to_graph(g)
    with open(CACHE_GRAPH_PATH, "wb") as f:
        pickle.dump(g, f)
set_graph(g)

print("âœ… Graph initialized")
print(g.nodes[list(g.nodes)[0]].keys())


# Cache pathways graph mapping
CACHE_PATHWAYS_PATH = "pathways_cache.pkl"
if os.path.exists(CACHE_PATHWAYS_PATH):
    with open(CACHE_PATHWAYS_PATH, "rb") as f:
        trip_graph, pathways_dict = pickle.load(f)
else:
    pathways = pd.read_csv('trip_pathways.csv')
    trip_graph = defaultdict(dict)
    pathways_dict = pathways.to_dict('index')
    for idx, row in pathways.iterrows():
        trip_graph[row['start_trip_id']][row['end_trip_id']] = idx 
    with open(CACHE_PATHWAYS_PATH, "wb") as f:
        pickle.dump((trip_graph, pathways_dict), f)
set_trip_graph(trip_graph, pathways_dict)




system_prompt = """
You are a smart assistant specialized in Alexandria public transportation. 
You have access to the following tools:

1. geocode_address(address) -> returns the latitude and longitude of the address.
2. get_nearest_node(lat, lon) -> returns the nearest OSM node ID.
3. explore_trips(source_node) -> returns all trips starting from this node, including walking distance.
4. find_journeys(start_trips, goal_trips) -> returns all possible journeys with path and costs (money, walking distance).
5. filter_best_journeys(journeys, max_results=5) -> returns the best journeys based on shortest walking distance and lowest cost.
6. format_journeys_for_user(journeys) -> returns a user-friendly Arabic description of the journeys.

You must always follow this workflow:
1. Find the coordinates of the start and destination using geocode_address. IMPORTANT: Always append ", Alexandria, Egypt" to the address provided by the user to ensure accuracy (e.g., if user says "Asafra", search for "Asafra, Alexandria, Egypt").
2. Convert each location into the nearest OSM node using get_nearest_node.
3. Explore trips from both start and destination nodes using explore_trips.
4. Find all possible journeys using find_journeys.
5. Filter the top journeys using filter_best_journeys.
6. Format the filtered journeys for the user using format_journeys_for_user.
7. Return only the final formatted journey description to the user. Do not return any intermediate data.

Output style requirements:
- Be clear, friendly, and concise in Arabic.
- Use headings, bullets, and icons (ğŸ›£ ğŸ’° ğŸš¶â€â™‚ï¸) similar to the tools output.
- Start with a brief confirmation of origin and destination, then list top journeys.
- For each journey: show the path (trip names), total price, and total walking distance.
- Avoid raw JSON; return a human-friendly formatted text only.
"""


def _regex_extract(query: str) -> tuple[str, str] | tuple[None, None]:
    text = re.sub(r"\s+", " ", query.strip())
    # Handle Arabic variants of "to": Ø¥Ù„Ù‰/Ø§Ù„Ù‰/Ù„Ù€/Ù„/Ù„ÙŠ
    m = re.search(r"Ù…Ù†\s+(.+?)\s+(?:Ø¥Ù„Ù‰|Ø§Ù„Ù‰|Ø§Ù„ÙŠ|Ø¥Ù„ÙŠ|Ù„Ù€|Ù„|Ù„ÙŠ)\s+(.+)$", text)
    if m:
        return m.group(1).strip(), m.group(2).strip()
    return None, None


def run_once(query: str) -> str:
    # 1) LLM parses the user query to origin/destination once (JSON-only)
    api_key = os.environ.get("GOOGLE_API_KEY", "")
    genai.configure(api_key=api_key)
    parse_prompt = (
        "Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù†ÙˆØ§ÙŠØ§. Ø£Ø®Ø±Ø¬ Ù…ÙƒØ§Ù† Ø§Ù„Ø§Ù†Ø·Ù„Ø§Ù‚ ÙˆØ§Ù„ÙˆØµÙˆÙ„ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ£Ø¹Ø¯ JSON ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ ÙƒÙ„Ø§Ù… Ø¥Ø¶Ø§ÙÙŠØŒ"
        " Ø¨Ø§Ù„Ù…ÙØªØ§Ø­ÙŠÙ† origin Ùˆ destinationØŒ Ù…Ø«Ø§Ù„: {\"origin\":\"...\",\"destination\":\"...\"}.\n\n"
        f"Ø§Ù„Ù†Øµ: {query}"
    )
    origin = None
    dest = None
    try:
        parse_resp = genai.GenerativeModel("gemini-pro").generate_content(parse_prompt, request_options={"retry": None, "timeout": 20})
        raw = getattr(parse_resp, "text", "") or ""
        # Extract first JSON object from text
        jmatch = re.search(r"\{[\s\S]*\}", raw)
        if jmatch:
            data = json.loads(jmatch.group(0))
            origin = (data.get("origin") or data.get("Ø§Ù„Ø§Ù†Ø·Ù„Ø§Ù‚") or "").strip() or None
            dest = (data.get("destination") or data.get("Ø§Ù„ÙˆØµÙˆÙ„") or "").strip() or None
    except Exception:
        pass

    # Fallback: regex extraction if LLM parse failed
    if not origin or not dest:
        r_origin, r_dest = _regex_extract(query)
        origin = origin or r_origin
        dest = dest or r_dest

    if not origin or not dest:
        return "Ù…Ù† ÙØ¶Ù„Ùƒ Ø­Ø¯Ù‘Ø¯ Ù…ÙƒØ§Ù† Ø§Ù„Ø§Ù†Ø·Ù„Ø§Ù‚ ÙˆØ§Ù„ÙˆØµÙˆÙ„ Ø¨ÙˆØ¶ÙˆØ­ØŒ Ù…Ø«Ù„: Ù…Ù† Ù…Ø­Ø·Ø© Ù…ØµØ± Ø¥Ù„Ù‰ Ø£Ø¨Ùˆ ÙŠÙˆØ³Ù."

    # 2) Tools pipeline (deterministic)
    src_geo = geocode_address(origin)
    dst_geo = geocode_address(dest)
    if "error" in src_geo or "error" in dst_geo:
        return "Ù„Ù… Ø£Ø³ØªØ·Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø¨Ø¯Ù‚Ø©. Ø¬Ø±Ù‘Ø¨ ØµÙŠØºØ© Ø£Ø®Ø±Ù‰." 
    src_node = get_nearest_node(src_geo["lat"], src_geo["lon"]) 
    dst_node = get_nearest_node(dst_geo["lat"], dst_geo["lon"]) 

    # Optional debug: show resolved coordinates and node ids
    if os.environ.get("DEBUG_ROUTING", "").strip():
        print(f"[DEBUG] origin='{origin}' -> lat={src_geo['lat']}, lon={src_geo['lon']}, node={src_node}")
        print(f"[DEBUG] dest='{dest}' -> lat={dst_geo['lat']}, lon={dst_geo['lon']}, node={dst_node}")
    start_trips = explore_trips(src_node)
    goal_trips = explore_trips(dst_node)
    journeys = find_journeys(start_trips, goal_trips)
    best = filter_best_journeys(journeys, max_results=5)
    formatted = format_journeys_for_user(best)

    # 3) Single LLM call for final Arabic answer, prefer 2.5-flash then fallback to pro
    polish_prompt = (
        f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ±ÙŠØ¯ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ù…Ù† {origin} Ø¥Ù„Ù‰ {dest}.\n\n" 
        "Ø£ÙƒØ¯ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ ÙˆÙ…ÙÙ‡ÙˆÙ…ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ù„Ù‡Ø¬ØªÙ‡ Ø§Ù„Ù…ØµØ±ÙŠØ© Ø¥Ù† Ø£Ù…ÙƒÙ†ØŒ"
        " Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙƒÙ…Ø§ Ù‡ÙŠ ØªÙ…Ø§Ù…Ù‹Ø§.\n\n" + formatted
    )
    try:
        resp = genai.GenerativeModel("gemini-2.5-flash").generate_content(polish_prompt, request_options={"retry": None, "timeout": 60})
        return getattr(resp, "text", str(resp))
    except Exception:
        try:
            resp = genai.GenerativeModel("gemini-pro").generate_content(polish_prompt, request_options={"retry": None, "timeout": 60})
            return getattr(resp, "text", str(resp))
        except Exception:
            return formatted

if __name__ == "__main__":
    user_query = "Ø£Ø±ÙŠØ¯ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„ÙŠ Ø§Ù„Ø¹ØµØ§ÙØ±Ø©"
    print(" Ø§Ù„Ø³Ø¤Ø§Ù„:", user_query)
    out = run_once(user_query)
    print(" Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print(out)
# agent.run("Ø£Ø±ÙŠØ¯ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ù…Ù† Ø§Ù„Ø¹Ø¬Ù…ÙŠ Ø¥Ù„Ù‰ Ù…Ø­Ø·Ø© Ø§Ù„Ø±Ù…Ù„")

# response = model.invoke("Ù‡Ùˆ Ø§Ø²Ø§ÙŠ Ø§Ø±ÙˆØ­ Ù…Ù† Ù…Ø­Ø·Ø© Ù…ØµØ± Ù„Ù„Ø¹Ø¬Ù…ÙŠ ØŸ ÙÙŠ Ù…Ø´Ø§Ø±ÙŠØ¹ Ø¨ØªØ±ÙˆØ­ Ù‡Ù†Ø§ÙƒØŸ")
# print(response.content)