from langchain.tools import tool
import google.generativeai as genai
from geopy.geocoders import Nominatim
import time
import pandas as pd
from models.trip_price_class import TripPricePredictor
from tools import *
from dotenv import load_dotenv
from pathlib import Path
import os
import re
import json

# Load environment variables
load_dotenv()

print(" DB-only mode: skipping OSM graph initialization")


print(" DB-only mode: skipping pathways graph caching")




system_prompt = """
You are a smart assistant specialized in Alexandria public transportation.
You must use DATABASE tools only:

1. search_stop_by_name_db(name) -> returns candidate stops from DB.
2. get_nearest_stop_db(lat, lon) -> returns nearest DB stop.
3. find_journeys_db(origin_stop_id, dest_stop_id) -> returns journeys (path, money, walk).
4. filter_best_journeys(journeys) -> filter top journeys.
5. format_journeys_for_user(journeys) -> Arabic formatted output.

Workflow:
1. Parse origin/destination via Gemini.
2. Resolve to DB stops and compute journeys via find_journeys_db.
3. Return formatted journeys only.

Output style requirements:
- Be clear, friendly, and concise in Arabic.
- Use headings, bullets, and icons (ğŸ›£ ğŸ’° ğŸš¶â€â™‚ï¸).
- Avoid raw JSON; return human-friendly text only.
"""


def _regex_extract(query: str) -> tuple[str, str] | tuple[None, None]:
    # Disabled: fallback extraction via regex is no longer used.
    return None, None


def run_once(query: str) -> str:
    # 1) LLM parses the user query to origin/destination once (JSON-only)
    api_key = os.environ.get("GOOGLE_API_KEY", "")
    genai.configure(api_key=api_key)
    parse_prompt = (
        "Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù†ÙˆØ§ÙŠØ§. Ø£Ø®Ø±Ø¬ Ù…ÙƒØ§Ù† Ø§Ù„Ø§Ù†Ø·Ù„Ø§Ù‚ ÙˆØ§Ù„ÙˆØµÙˆÙ„ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ£Ø¹Ø¯ JSON ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ ÙƒÙ„Ø§Ù… Ø¥Ø¶Ø§ÙÙŠØŒ"
        " Ø§Ø³ØªØ¹Ù…Ù„ Ø§Ù„Ù…ÙØªØ§Ø­ÙŠÙ† Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ØªÙ…Ø§Ù…Ù‹Ø§: origin Ùˆ destination. Ø£Ø¹ÙØ¯Ù‘ JSON Ù…Ø¶ØºÙˆØ· Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ Ø¨Ø¯ÙˆÙ† Ø£Ø³Ø·Ø± Ø¬Ø¯ÙŠØ¯Ø© ÙˆÙ„Ø§ ØªØ¹Ù„ÙŠÙ‚Ø§Øª"
        " ÙˆØ¨Ø¯ÙˆÙ† Ø£Ù‚ÙˆØ§Ø³ Ø£Ùˆ ØªÙ†Ø³ÙŠÙ‚ Ø¥Ø¶Ø§ÙÙŠ Ù…Ø«Ù„ ``` Ø£Ùˆ ```json. Ù…Ø«Ø§Ù„ Ø¯Ù‚ÙŠÙ‚: {\"origin\":\"Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯\",\"destination\":\"Ø§Ù„Ø¹ØµØ§ÙØ±Ø©\"}.\n\n"
        f"Ø§Ù„Ù†Øµ: {query}"
    )
    origin = None
    dest = None
    try:
        # Use a modern fast model for parsing
        parse_resp = genai.GenerativeModel("gemini-2.5-flash").generate_content(parse_prompt, request_options={"retry": None, "timeout": 20})
        raw = getattr(parse_resp, "text", "") or ""
        # Strip common code fences/backticks and language tags
        raw = re.sub(r"^```[a-zA-Z]*\n|\n```$", "", raw.strip())
        raw = raw.strip()
        # Extract first JSON object from text
        jmatch = re.search(r"\{[\s\S]*\}", raw)
        if jmatch:
            data = json.loads(jmatch.group(0))
            origin = (data.get("origin") or data.get("Ø§Ù„Ø§Ù†Ø·Ù„Ø§Ù‚") or "").strip() or None
            dest = (data.get("destination") or data.get("Ø§Ù„ÙˆØµÙˆÙ„") or "").strip() or None
    except Exception:
        pass

    # Require valid LLM parse; no regex fallback
    if not origin or not dest:
        return "ØªØ¹Ø°Ù‘Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ø§Ù†Ø·Ù„Ø§Ù‚ ÙˆØ§Ù„ÙˆØµÙˆÙ„ Ø¹Ø¨Ø± Gemini. ØªØ£ÙƒÙ‘Ø¯ Ù…Ù† ÙƒØªØ§Ø¨Ø© Ø§Ù„ØµÙŠØºØ© Ø¨ÙˆØ¶ÙˆØ­ (Ù…Ø«Ø§Ù„: Ù…Ù† [Ø§Ù„Ù…ÙƒØ§Ù† A] Ø¥Ù„Ù‰ [Ø§Ù„Ù…ÙƒØ§Ù† B]) ÙˆØ¨ÙˆØ¬ÙˆØ¯ Ù…ÙØªØ§Ø­ API ØµØ§Ù„Ø­."

    # 2) Tools pipeline (deterministic)
    # Prefer DB stop name search first; else use geocoder
    db_first = os.environ.get("USE_DB", "").strip()
    found_src = search_stop_by_name_db(origin) if db_first else []
    found_dst = search_stop_by_name_db(dest) if db_first else []
    if found_src:
        src_geo = {"lat": found_src[0]["lat"], "lon": found_src[0]["lon"], "db_stop_id": found_src[0]["stop_id"]}
    else:
        src_geo = geocode_address(origin)
    if found_dst:
        dst_geo = {"lat": found_dst[0]["lat"], "lon": found_dst[0]["lon"], "db_stop_id": found_dst[0]["stop_id"]}
    else:
        dst_geo = geocode_address(dest)
    if "error" in src_geo or "error" in dst_geo:
        return "Ù„Ù… Ø£Ø³ØªØ·Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø¨Ø¯Ù‚Ø©. Ø¬Ø±Ù‘Ø¨ ØµÙŠØºØ© Ø£Ø®Ø±Ù‰." 
    # Try DB nearest stop to verify correctness; fallback to OSM node
    db_near_src = get_nearest_stop_db(src_geo["lat"], src_geo["lon"]) if db_first else None
    db_near_dst = get_nearest_stop_db(dst_geo["lat"], dst_geo["lon"]) if db_first else None
    src_node = get_nearest_node(src_geo["lat"], src_geo["lon"]) 
    dst_node = get_nearest_node(dst_geo["lat"], dst_geo["lon"]) 

    # Optional debug: show resolved coordinates and node ids
    if os.environ.get("DEBUG_ROUTING", "").strip():
        print(f"[DEBUG] origin='{origin}' -> lat={src_geo['lat']}, lon={src_geo['lon']}, node={src_node}, db_nearest={db_near_src}")
        print(f"[DEBUG] dest='{dest}' -> lat={dst_geo['lat']}, lon={dst_geo['lon']}, node={dst_node}, db_nearest={db_near_dst}")
    # Prefer DB journeys if DB is enabled and nearest DB stops are available
    best = []
    if db_first and db_near_src and db_near_dst:
        db_journeys = find_journeys_db(db_near_src["stop_id"], db_near_dst["stop_id"], max_results=5)
        if db_journeys:
            best = filter_best_journeys(db_journeys, max_results=5)

    # NO FALLBACK: Database only mode
    if not best:
        return " Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø±Ø§Øª Ù…ØªØ§Ø­Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø­Ø·Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…."

    # Persist journeys to JSON for later querying
    try:
        out_dir = Path("output"); out_dir.mkdir(parents=True, exist_ok=True)
        payload = {
            "origin": origin,
            "destination": dest,
            "origin_geo": src_geo,
            "destination_geo": dst_geo,
            "start_node": src_node,
            "dest_node": dst_node,
            "journeys": [
                {
                    "path": j["path"],
                    "decoded_path": [decode_trip(t) if isinstance(t, (int, float)) or str(t).isdigit() else str(t) for t in j["path"]],
                    "costs": j["costs"],
                    "transfers": max(0, len(j["path"]) - 1)
                } for j in best
            ]
        }
        with open(out_dir / "journeys.json", "w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

    formatted = format_journeys_for_user(best)

    # 3) Single LLM call for final Arabic answer, prefer 2.5-flash then fallback to pro
    polish_prompt = (
        f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ±ÙŠØ¯ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ù…Ù† {origin} Ø¥Ù„Ù‰ {dest}.\n\n" 
        "Ø£ÙƒØ¯ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ ÙˆÙ…ÙÙ‡ÙˆÙ…ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ù„Ù‡Ø¬ØªÙ‡ Ø§Ù„Ù…ØµØ±ÙŠØ© Ø¥Ù† Ø£Ù…ÙƒÙ†ØŒ"
        " Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙƒÙ…Ø§ Ù‡ÙŠ ØªÙ…Ø§Ù…Ù‹Ø§.\n\n" + formatted
    )
    try:
        resp = genai.GenerativeModel("gemini-2.5-flash").generate_content(polish_prompt, request_options={"retry": None, "timeout": 60})
        return getattr(resp, "text", str(resp))
    except Exception:
        try:
            resp = genai.GenerativeModel("gemini-2.5-flash").generate_content(polish_prompt, request_options={"retry": None, "timeout": 60})
            return getattr(resp, "text", str(resp))
        except Exception:
            return formatted


def query_saved_journeys() -> dict | None:
    """Load last saved journeys from output/journeys.json."""
    p = Path("output/journeys.json")
    if not p.exists():
        return None
    try:
        with open(p, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

if __name__ == "__main__":
    user_query = "Ø£Ø±ÙŠØ¯ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„ÙŠ Ø§Ù„Ø¹ØµØ§ÙØ±Ø©"
    print(" Ø§Ù„Ø³Ø¤Ø§Ù„:", user_query)
    out = run_once(user_query)
    print(" Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print(out)
    saved = query_saved_journeys()
    if saved:
        print("\n[Saved] output/journeys.json written with current results.")
# agent.run("Ø£Ø±ÙŠØ¯ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ù…Ù† Ø§Ù„Ø¹Ø¬Ù…ÙŠ Ø¥Ù„Ù‰ Ù…Ø­Ø·Ø© Ø§Ù„Ø±Ù…Ù„")

# response = model.invoke("Ù‡Ùˆ Ø§Ø²Ø§ÙŠ Ø§Ø±ÙˆØ­ Ù…Ù† Ù…Ø­Ø·Ø© Ù…ØµØ± Ù„Ù„Ø¹Ø¬Ù…ÙŠ ØŸ ÙÙŠ Ù…Ø´Ø§Ø±ÙŠØ¹ Ø¨ØªØ±ÙˆØ­ Ù‡Ù†Ø§ÙƒØŸ")
# print(response.content)